{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fa4a448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pysrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f86a79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5fa3d30",
   "metadata": {},
   "source": [
    "## Описание задачи"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8c67d64",
   "metadata": {},
   "source": [
    "Даны списки субтитров и категории фильмов в зависимости от сложности: уровни A1,A2,B1,B2,C1.\n",
    "\n",
    "На основании данных необходимо построить модель, которая будет определять уровень фильма на основании субтитров.\n",
    "\n",
    "Данная задача относится к задачам мультиклассификации.\n",
    "\n",
    "В связи с задачей мультиклассификации, метрика данной работы f1_score.\n",
    "\n",
    "В работе будут использованы две модели - log.regression и catboost.\n",
    "\n",
    "По завершению будет сохранен дамп модели.\n",
    "\n",
    "Первоначально будет собран общий датасет на основании субтитров. После чего он будет очищен от дубликатов с фиксацией минимального уровня. Далее будет произведен EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f898e00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#импорт библиотек\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pysrt\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer,CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "from pickle import dump"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "286725c2",
   "metadata": {},
   "source": [
    "## 1.Создание исходного датафрейма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fb3de657",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файлы и директории в  C:/Users/angel/project_english_school/Subtitles_all_new/rar :\n",
      "['01_Extra_English_-_Hectors_arrival.srt', '02_Extra_English_-_Hector_goes_shopping.Vie_Syned.srt', '03_Extra_English_-_Hector_has_a_date.Eng_Syned.srt', '04_Extra_English_-_Hector_looks_for_a_job.Vie_Syned.srt', '05_Extra_English_-_A_star_is_born.Eng_Syned.srt', '06_Extra_English_-_Bridget_wins_the_lottery.Vie_Syned.srt', '07_Extra_English_-_The_twin.Eng_Syned.srt', '08_Extra_English_-_The_landladys_cousin.Vie_Syned.srt', '09_Extra_English_-_Jobs_for_the_boys.Eng_Syned.srt', '10_Cloverfield_lane(2016).srt', '10_Extra_English_-_Annies_Protest.Vie_Syned.srt', '10_things_I_hate_about_you(1999).srt', '11_Extra_English_-_Holiday_time.Eng_Syned.srt', '12_Extra_English_-_Football_Crazy.Vie_Syned.srt', '13.Reasons.Why.S01E01.720p.WEBRiP.x265.ShAaNiG.srt', '13.Reasons.Why.S01E02.720p.WEBRiP.x265.ShAaNiG.srt', '13.Reasons.Why.S01E03.720p.WEBRiP.x265.ShAaNiG.srt', '13.Reasons.Why.S01E04.720p.WEBRiP.x265.ShAaNiG.srt', '13.Reasons.Why.S01E05.720p.WEBRiP.x265.ShAaNiG.srt', '13.Reasons.Why.S01E06.720p.WEBRiP.x265.ShAaNiG.srt', '13.Reasons.Why.S01E07.720p.WEBRiP.x265.ShAaNiG.srt', '13.Reasons.Why.S01E08.720p.WEBRiP.x265.ShAaNiG.srt', '13.Reasons.Why.S01E09.720p.WEBRiP.x265.ShAaNiG.srt', '13.Reasons.Why.S01E10.720p.WEBRiP.x265.ShAaNiG.srt', '13.Reasons.Why.S01E11.720p.WEBRiP.x265.ShAaNiG.srt', '13.Reasons.Why.S01E12.720p.WEBRiP.x265.ShAaNiG.srt', '13.Reasons.Why.S01E13.720p.WEBRiP.x265.ShAaNiG.srt', '13_Extra_English_-_A_wedding_in_the_air.Eng_Syned.srt', '14_Extra_English_-_Changes.Vie_Syned.srt', '15_Extra_English_-_The_bouncer.Eng_Syned.srt', '16_Extra_English_-_Uncle_Nick.Vie_Syned.srt', '17_Extra_English_-_Cyber_Stress.Eng_Syned.srt', '18_Extra_English_-_Just_the_ticket.Vie_Syned.srt', '19_Extra_English_-_Kung_fu_fighting.Eng_Syned.srt', '20_Extra_English_-_Every_dog_has_its_day.Vie_Syned.srt', '21_Extra_English_-_The_entertainers.Eng_Syned.srt', '22_Extra_English_-_Haunting_at_Halloween.Vie_Syned.srt', '23_Extra_English_-_Truth_or_dare.Eng_Syned.srt', '24_Extra_English_-_Pilot_Nick.Vie_Syned.srt', '25_Extra_English_-_Art.Eng_Syned.srt', '26_Extra_English_-_Alibi.Vie_Syned.srt', '27_Extra_English_-_Can_you_live_without.Eng_Syned.srt', '28_Extra_English_-_Christmas.Vie_Syned.srt', '29_Extra_English_-_Camping.Eng_Syned.srt', '30_Extra_English_-_Love_hurts.Vie_Syned.srt', 'Adventure.Time.Distant.Lands.S01E02.1080p.WEB.h264-OPUS.srt', 'Aladdin(1992).srt', 'ALF S1 E1.srt', 'ALF S1 E10.srt', 'ALF S1 E2.srt', 'ALF S1 E3.srt', 'ALF S1 E4.srt', 'ALF S1 E5.srt', 'ALF S1 E6.srt', 'ALF S1 E7.srt', 'ALF S1 E8.srt', 'ALF S1 E9.srt', 'All_dogs_go_to_heaven(1989).srt', 'American.History.X.1998.1080p.BluRay.x264.YIFY.srt', 'AmericanBeauty1999.BRRip.srt', \"Angela's.Christmas.2018.WEBRip.Netflix.srt\", 'Angelas.Christmas.Wish.2020.srt', 'An_American_tail(1986).srt', 'A_knights_tale(2001).srt', 'A_star_is_born(2018).srt', 'Babe(1995).srt', 'Back_to_the_future(1985).srt', 'Banking_On_Bitcoin(2016).srt', 'Batman_begins(2005).srt', 'Beauty_and_the_beast(2017).srt', 'Before_I_go_to_sleep(2014).srt', 'Before_sunrise(1995).srt', 'Before_sunset(2004).srt', 'Billions.S01E08.HDTV.x264-KILLERS.srt', 'Billions.S01E09.HDTV.x264-FLEET.srt', 'Billions.S01E10.720p.HDTV.x264-AVS.srt', 'Billions.S01E11.HDTV.x264-FLEET.srt', 'Billions.S01E12.HDTV.x264-BATV.srt', 'Bones - 1x01 Pilot.srt', 'Bones - 1x02 The Man in the SUV.srt', 'Bones - 1x03 A Boy in the Tree.srt', 'Bones - 1x04 The Man in the Bear.srt', 'Bones - 1x05 A Boy In a Bush.srt', 'Braveheart(1995).srt', \"Breakfast.at.Tiffany's.1961.720p.BRRip.x264.AAC-ETRG.srt\", 'Breaking_Bad_The_Movie(2017).srt', 'BrenВ.Brown.The.Call.to.Courage.2019.720.NF.720p.DDP.5.1.x264-CafeFlix.srt', 'Bridget_Joness_Baby.srt', 'Bridget_Jones_diary(2001).srt', 'Bullet train.srt', 'Cars(2006).srt', 'Casper.1995.1080p.BluRay.X264-AMIABLE.srt', 'Casper.srt', 'Cast_away(2000).srt', 'Catch_me_if_you_can(2002).srt', 'Charlie_and_the_Chocolate_Factory.srt', 'Chip S1 E1.srt', 'Chip S1 E2.srt', 'Chip S1 E3.srt', 'Chip S1 E4.srt', 'Chip S1 E5.srt', 'Cinderella(1950).srt', 'Clueless(1995).srt', 'Collateral.Beauty.2016.720p.BRRip.x264.AAC-ETRG.srt', 'Crazy4TV.com - Suits.S06E01.720p.BluRay.x265.HEVC.Crazy4ad.srt', 'Crazy4TV.com - Suits.S06E02.720p.BluRay.x265.HEVC.Crazy4ad.srt', 'Crazy4TV.com - Suits.S06E03.720p.BluRay.x265.HEVC.Crazy4ad.srt', 'Crazy4TV.com - Suits.S06E04.720p.BluRay.x265.HEVC.Crazy4ad.srt', 'Crazy4TV.com - Suits.S06E05.720p.BluRay.x265.HEVC.Crazy4ad.srt', 'Crazy4TV.com - Suits.S06E06.720p.BluRay.x265.HEVC.Crazy4ad.srt', 'Crazy4TV.com - Suits.S06E07.720p.BluRay.x265.HEVC.Crazy4ad.srt', 'Crazy4TV.com - Suits.S06E08.720p.BluRay.x265.HEVC.Crazy4ad.srt', 'Crazy4TV.com - Suits.S06E09.720p.BluRay.x265.HEVC.Crazy4ad.srt', 'Crazy4TV.com - Suits.S06E10.720p.BluRay.x265.HEVC.Crazy4ad.srt', 'Crazy4TV.com - Suits.S06E11.720p.BluRay.x265.HEVC.Crazy4ad.srt', 'Crazy4TV.com - Suits.S06E12.720p.BluRay.x265.HEVC.Crazy4ad.srt', 'Crazy4TV.com - Suits.S06E13.720p.BluRay.x265.HEVC.Crazy4ad.srt', 'Crazy4TV.com - Suits.S06E14.720p.BluRay.x265.HEVC.Crazy4ad.srt', 'Crazy4TV.com - Suits.S06E15.720p.BluRay.x265.HEVC.Crazy4ad.srt', 'Crazy4TV.com - Suits.S06E16.720p.BluRay.x265.HEVC.Crazy4ad.srt', 'Crown, The S01E01 - Wolferton Splash.en.SDH.srt', 'Crown, The S01E01 - Wolferton Splash.en.srt', 'Crown, The S01E02 - Hyde Park Corner.en.SDH.srt', 'Crown, The S01E02 - Hyde Park Corner.en.srt', 'Crown, The S01E03 - Windsor.en.FORCED.srt', 'Crown, The S01E03 - Windsor.en.SDH.srt', 'Crown, The S01E03 - Windsor.en.srt', 'Crown, The S01E04 - Act of God.en.SDH.srt', 'Crown, The S01E04 - Act of God.en.srt', 'Crown, The S01E05 - Smoke and Mirrors.en.FORCED.srt', 'Crown, The S01E05 - Smoke and Mirrors.en.SDH.srt', 'Crown, The S01E05 - Smoke and Mirrors.en.srt', 'Crown, The S01E06 - Gelignite.en.SDH.srt', 'Crown, The S01E06 - Gelignite.en.srt', 'Crown, The S01E07 - Scientia Potentia Est.en.FORCED.srt', 'Crown, The S01E07 - Scientia Potentia Est.en.SDH.srt', 'Crown, The S01E07 - Scientia Potentia Est.en.srt', 'Crown, The S01E08 - Pride & Joy.en.SDH.srt', 'Crown, The S01E08 - Pride & Joy.en.srt', 'Crown, The S01E09 - Assassins.en.SDH.srt', 'Crown, The S01E09 - Assassins.en.srt', 'Crown, The S01E10 - Gloriana.en.FORCED.srt', 'Crown, The S01E10 - Gloriana.en.SDH.srt', 'Crown, The S01E10 - Gloriana.en.srt', 'Deadpool(2016).srt', 'Desperate Housewives - Episode 1.01 - Pilot.srt', 'Desperate Housewives - Episode 1.02 - Ah, but underneath.srt', 'Desperate Housewives - Episode 1.03 - Pretty little picture.srt', \"Desperate Housewives - Episode 1.04 - Who's that woman.srt\", 'Desperate Housewives - Episode 1.05 - Come in, stranger.srt', 'Despicable_Me(2010).srt', 'Die_hard(1988).srt', 'Downton Abbey - S01E01 - Episode 1.eng.SDH.srt', 'Downton Abbey - S01E02 - Episode 2.eng.SDH.srt', 'Downton Abbey - S01E03 - Episode 3.eng.SDH.srt', 'Downton Abbey - S01E04 - Episode 4.eng.SDH.srt', 'Downton Abbey - S01E05 - Episode 5.eng.SDH.srt', 'Downton Abbey - S01E06 - Episode 6.eng.SDH.srt', 'Downton Abbey - S01E07 - Episode 7.eng.SDH.srt', 'Dredd(2012).srt', 'Dune(2021).srt', 'Eat.Pray.Love.2010.DC.1080p.BluRay.x264.DTS-FGT_track3_eng.srt', 'Enola_Holmes(2020).srt', 'Entrapment.srt', 'EternalSunshineOfTheSpotlessMind.2004.BrRip.srt', 'Eurovision_song_contest_(2020).srt', 'Ferdinand(2017).srt', 'Fight_club(1999).srt', 'Finding_Nemo(2003).srt', 'Forrest_Gump(1994).srt', 'Frozen.2013.WEB-DL.DSNP.srt', 'Frozen.Fever.2015.720p.BluRay.x264.AAC-ETRG.srt', 'Game.Of.Thrones.S01E01.Winter.Is.Coming.HDTV.XviD-FEVER.HI.srt', 'Game.Of.Thrones.S01E02.The.Kingsroad.HDTV.XviD-FQM.srt', 'Game.Of.Thrones.S01E03.Lord.Snow.HDTV.XviD-FQM.srt', 'Game.Of.Thrones.S01E04.Cripples, Bastards, and Broken Things.HDTV.XviD-FQM.srt', 'Game.Of.Thrones.S01E05.The.Wolf.and.the.Lion.HDTV.XviD-FQM.srt', 'garfield-the-movie_english-972167 (2004).srt', 'Ghosts.of.Girlfriends.Past.2009.BluRay.720p.x264.YIFY.srt', 'Gogo_Loves_English.srt', 'Gone.With.The.Wind.1939.720p.BluRay.x264-SiNNERS.srt', 'Good_Will_Hunting(1997).srt', 'Gravity Falls 2x18 Weirdmageddon Part I.srt', 'Gravity.Falls.S01E01.Tourist.Trapped.720p.WEB-DL.AAC2.0.H264-Reaperza.srt', 'Gravity.Falls.S01E02.The.Legend.of.Gobblewonker.720p.WEB-DL.AAC2.0.H264-Reaperza.srt', 'Gravity.Falls.S01E03.Headhunters.720p.WEB-DL.AAC2.0.H264-Reaperza.srt', 'Gravity.Falls.S01E04.The.Hand.That.Rocks.the.Mabel.720p.WEB-DL.AAC2.0.H264-Reaperza.srt', 'Gravity.Falls.S01E05.The.Inconveniencing.720p.WEB-DL.AAC2.0.H264-Reaperza.srt', 'Gravity.Falls.S01E06.Dipper.vs.Manliness.720p.WEB-DL.AAC2.0.H264-Reaperza.srt', 'Gravity.Falls.S01E07.Double.Dipper.720p.WEB-DL.AAC2.0.H264-Reaperza.srt', 'Gravity.Falls.S01E08.Irrational.Treasure.720p.WEB-DL.AAC2.0.H264-Reaperza.srt', 'Gravity.Falls.S01E09.Time.Travelers.Pig.720p.WEB-DL.AAC2.0.H264-Reaperza.srt', 'Gravity.Falls.S01E10.Fight.Fighters.720p.WEB-DL.AAC2.0.H264-Reaperza.srt', 'Gravity.Falls.S01E11.Little.Dipper.720p.WEB-DL.AAC2.0.H264-Reaperza.srt', 'Gravity.Falls.S01E12.Summerween.720p.WEB-DL.AAC2.0.H264-Reaperza.srt', 'Gravity.Falls.S01E13.Boss.Mabel.720p.WEB-DL.AAC2.0.H.264-iT00NZ.srt', 'Gravity.Falls.S01E14.Bottomless.Pit.720p.WEB-DL.AAC2.0.H.264-iT00NZ.srt', 'Gravity.Falls.S01E15.The.Deep.End.720p.WEB-DL.AAC2.0.H.264-iT00NZ.srt', 'Gravity.Falls.S01E16.Carpet.Diem.720p.WEB-DL.AAC2.0.H.264-iT00NZ.srt', 'Gravity.Falls.S01E17.Boyz.Crazy.720p.WEB-DL.AAC2.0.H.264-iT00NZ.srt', 'Gravity.Falls.S01E18.Land.Before.Swine.720p.WEB-DL.AAC2.0.H.264-iT00NZ.srt', 'Gravity.Falls.S01E19.Dreamscaperers.720p.WEB-DL.AAC2.0.H.264-iT00NZ.srt', 'Gravity.Falls.S01E20.Gideon.Rises.720p.WEB-DL.AAC2.0.H.264-iT00NZ.srt', 'Groundhog_day(1993).srt', 'Harry_Potter_and_the_philosophers_stone(2001).srt', 'Her(2013).srt', 'Home_alone(1990).srt', 'Hook(1991).srt', 'House.of.Cards.S01E01.House.Of.Cards.Part.1.720p.BluRay.x264-DEiMOS.srt', 'House.of.Cards.S01E02.House.Of.Cards.Part.2.720p.BluRay.x264-DEiMOS.srt', 'House.of.Cards.S01E03.House.Of.Cards.Part.3.720p.BluRay.x264-DEiMOS.srt', 'House.of.Cards.S01E04.House.Of.Cards.Part.4.720p.BluRay.x264-DEiMOS.srt', 'House.of.Cards.S02E01.To.Play.The.King.Part.1.720p.BluRay.x264-DEiMOS.srt', 'House.S01E01.Pilot..Everybody.Lies.DVDRip.XviD-FoV.EN.srt', 'House.S01E01.Pilot..Everybody.Lies.DVDRip.XviD-FoV.Eng.srt', 'House.S01E02.Paternity.DVDRip.XviD-FoV.EN.srt', 'House.S01E02.Paternity.DVDRip.XviD-FoV.Eng.srt', \"House.S01E03.Occam's.Razor.DVDRip.XviD-FoV.EN.srt\", 'House_of_Gucci(2021).srt', 'How I Met Your Mother - 1x01 - Pilot.srt', 'How I Met Your Mother - 1x02 - Purple Giraffe.srt', 'How I Met Your Mother - 1x03 - Sweet Taste of Liberty.srt', 'How I Met Your Mother - 1x04 - Return Of The Shirt.srt', 'How I Met Your Mother - 1x05 - Okay Awsome.srt', 'I.Am.Not.Okay.With.This.S01E01.Dear.Diary.720p.NF.WEB-DL.DDP5.1.x264-NTG.srt', 'I.Am.Not.Okay.With.This.S01E02.The.Master.of.One.Fk.720p.NF.WEB-DL.DDP5.1.x264-NTG.srt', 'I.Am.Not.Okay.With.This.S01E03.The.Partys.Over.720p.NF.WEB-DL.DDP5.1.x264-NTG.srt', 'I.Am.Not.Okay.With.This.S01E04.Stan.by.Me.720p.NF.WEB-DL.DDP5.1.x264-NTG.srt', 'I.Am.Not.Okay.With.This.S01E05.Another.Day.in.Paradise.720p.NF.WEB-DL.DDP5.1.x264-NTG.srt', 'I.Am.Not.Okay.With.This.S01E06.Like.Father.Like.Daughter.720p.NF.WEB-DL.DDP5.1.x264-NTG.srt', 'I.Am.Not.Okay.With.This.S01E07.Deepest.Darkest.Secret.720p.NF.WEB-DL.DDP5.1.x264-NTG.srt', 'icarus.2017.web.x264-strife.srt', 'Indecent Proposal (1993).srt', 'Indiana Jones And The Last Crusade DVDRip Xvid -IZON-.srt', 'Inside_out(2015).srt', 'Interstellar-2014-HDCAM-NEW-SOURCE-READNFO-XVID-AC3-ACAB.srt', 'Its_a_wonderful_life(1946).srt', 'Klaus(2019).srt', 'Knives_out(2019).srt', 'Kubo_and_the_two_strings(2016).srt', 'Liar_liar(1997).srt', 'Lightyear.2022.1080p.WEBRip.x264-RARBG.srt', 'Lion(2016).srt', 'Logan(2017).srt', 'Love_actually(2003).srt', 'Made_of_Honor(2008).srt', 'Mamma.Mia.2008.2160p.BluRay.Remux_eng.srt', 'Mamma_Mia(2008).srt', 'Mary.Poppins.Returns.2019.DVDRip.XviD.AC3-EVO.srt', 'Mary_Poppins_returns(2018).srt', 'Matilda(1996).srt', 'mechanic-resurrection_.srt', 'Meet_the_parents(2000).srt', 'Men.In.Black.1997.720p.Bluray.x264-SEPTiC.srt', 'mikes.new.car.2002.720p.bluray.x264-sinners.eng.srt', 'Milada(2017).srt', 'Miracle On 34th Street 1994 BRrip [ResourceRG H264 by Cronus].srt', 'Modern.Family.S01E01.Pilot.720p.WEB-DL.h.264-LP.srt', 'Modern.Family.S01E02.The.Bicycle.Thief.720p.WEB-DL.h.264.DD5.1-LP.srt', 'Modern.Family.S01E03.Changes.720p.WEB-DL.h.264.DD5.1-LP.srt', 'Modern.Family.S01E04.The.Incident.720p.WEB-DL.H.264.DD5.1-MMI.srt', 'Modern.Family.S01E05.Coal.Digger.720p.WEB-DL.H.264.DD5.1-MMI.srt', 'Mona_Lisa_Smile(2003).srt', 'Moulin_Rouge(2001).srt', 'Mrs_Doubtfire(1993).srt', 'Muzzy in Gondoland Part1 -English.srt', 'Muzzy in Gondoland Part2 -English.srt', 'Muzzy in Gondoland Part3 -English.srt', 'Muzzy in Gondoland Part4 -English.srt', 'Muzzy in Gondoland Part5 -English.srt', 'Muzzy in Gondoland Part6 -English.srt', 'My.Big.Fat.Greek.Wedding.2002.720p.BluRay.x264.YIFY.srt', 'My_big_fat_Greek_wedding(2002).srt', 'Notting_Hill(1999).srt', 'Oceans_Eleven(2001).srt', 'Oceans_Twelve(2004).srt', 'Peppa.Pig.S01E01.Muddy.Puddles.720p.[itoons.ir].srt', 'Pirates_of_the_Caribbean(2003).srt', 'Pleasantville(1998).srt', 'Pocahontas.1995.iNTERNAL.BDRip.x264-PiER.srt', 'Powder(1995).srt', 'Pride_and_Prejudice.srt', 'Pulp.Fiction.1994.BluRay.1080p.5.1CH.x264.Ganool.com.srt', 'Pulp_fiction(1994).srt', 'Rat.Race.2001.1080p.WEB-DL.DD5.1.H264-FGT.srt', 'Ratatouille(2007).srt', 'Ready_or_not(2019).srt', 'Requiem.For.A.Dream.DC.2000.720p.BrRip.x264.YIFY.srt', 'Roman Holiday 1953 1080p WEBRip HEVC AAC.srt', \"Secrets Of Her Majesty's Secret Service eng.srt\", 'Seven.Worlds.One.Planet.S01E01.2160p.BluRay.Remux.eng.srt', 'Seven.Worlds.One.Planet.S01E02.2160p.BluRay.Remux.eng.srt', 'Seven.Worlds.One.Planet.S01E03.2160p.BluRay.Remux.eng.srt', 'Seven.Worlds.One.Planet.S01E04.2160p.BluRay.Remux.eng.srt', 'Seven.Worlds.One.Planet.S01E05.2160p.BluRay.Remux.eng.srt', 'Seven.Worlds.One.Planet.S01E06.2160p.BluRay.Remux.eng.srt', 'Seven.Worlds.One.Planet.S01E07.2160p.BluRay.Remux.eng.srt', 'Sex And The City - 101 - Sex And The City [RavyDavy].srt', 'Sex And The City - 102 - Models And Mortals [RavyDavy].srt', 'Sex And The City - 103 - Bay Of Married Pigs [RavyDavy] .srt', 'Sex And The City - 104 - Valley Of The Twenty-Something Guys [RavyDavy].srt', 'Sex And The City - 105 - The Power Of Female Sex [RavyDavy].srt', 'Sherlock.S01E00.DVDRip.XviD-HAGGiS (unaired pilot).srt', 'Sherlock.S01E01.DVDRip.XviD-HAGGiS.srt', 'Sherlock.S01E02.DVDRip.XviD-HAGGiS.srt', 'Sherlock.S01E03.DVDRip.XviD-HAGGiS.srt', 'Sherlock.S02E01.DVDRip.XviD-HAGGiS.srt', 'Sherlock.S02E02.DVDRip.XviD-HAGGiS.srt', 'Sherlock.S02E03.BDRip.XviD-HAGGiS.srt', 'Shrek(2001).srt', 'Sleepless_in_Seattle(1993).srt', 'SlingShot (2014) WEB.eng.srt', 'Smallville.S01E13.Kinetic.720p.BluRay.DDP5.1.x264-ILCY.srt', 'SOMM.Into.the.Bottle.2015.1080p.BluRay.x265-RARBG.en.srt', 'SOMM.Into.the.Bottle.2015.1080p.BluRay.x265-RARBG.en.srt.srt', 'Soul(2020).srt', 'Spirit.Stallion.of.the.Cimarron.EN.srt', 'Suits S04E01 EngSub.srt', 'Suits S04E02 EngSub.srt', 'Suits S04E03 EngSub.srt', 'Suits S04E04 EngSub.srt', 'Suits S04E05 EngSub.srt', 'Suits S04E06 EngSub.srt', 'Suits S04E07 EngSub.srt', 'Suits S04E08 EngSub.srt', 'Suits S04E09 EngSub.srt', 'Suits S04E10 EngSub.srt', 'Suits S04E11 EngSub.srt', 'Suits S04E12 EngSub.srt', 'Suits S04E13 EngSub.srt', 'Suits S04E14 EngSub.srt', 'Suits S04E15 EngSub.srt', 'Suits S04E16 EngSub.srt', 'Suits.Episode 1- Denial.srt', 'Suits.Episode 10- Faith.srt', 'Suits.Episode 11- Blowback.srt', 'Suits.Episode 12- Live to Fight.srt', \"Suits.Episode 13- God's Green Earth.srt\", 'Suits.Episode 14- Self Defense.srt', 'Suits.Episode 15- Tick Tock.srt', 'Suits.Episode 16- 25th Hour.srt', 'Suits.Episode 2- Compensation.srt', 'Suits.Episode 3- No Refills.srt', 'Suits.Episode 4- No Puedo Hacerlo.srt', 'Suits.Episode 5- Toe to Toe.srt', 'Suits.Episode 6- Privilege.srt', 'Suits.Episode 7- Hitting Home.srt', 'Suits.Episode 8- Mea Culpa.srt', 'Suits.Episode 9- Uninvited Guests.srt', 'Suits.S01E01.1080p.BluRay.AAC5.1.x265-DTG.02.EN.srt', 'Suits.S01E02.1080p.BluRay.AAC5.1.x265-DTG.02.EN.srt', 'Suits.S01E03.1080p.BluRay.AAC5.1.x265-DTG.02.EN.srt', 'Suits.S01E04.1080p.BluRay.AAC5.1.x265-DTG.02.EN.srt', 'Suits.S01E05.1080p.BluRay.AAC5.1.x265-DTG.02.EN.srt', 'Suits.S01E06.1080p.BluRay.AAC5.1.x265-DTG.02.EN.srt', 'Suits.S01E07.1080p.BluRay.AAC5.1.x265-DTG.02.EN.srt', 'Suits.S01E08.1080p.BluRay.AAC5.1.x265-DTG.02.EN.srt', 'Suits.S01E09.1080p.BluRay.AAC5.1.x265-DTG.02.EN.srt', 'Suits.S01E10.1080p.BluRay.AAC5.1.x265-DTG.02.EN.srt', 'Suits.S01E11.1080p.BluRay.AAC5.1.x265-DTG.02.EN.srt', 'Suits.S01E12.1080p.BluRay.AAC5.1.x265-DTG.02.EN.srt', 'Suits.S02E01.HDTV.x264-AVS.srt', 'Suits.S02E02.HDTV.x264-ASAP.srt', 'Suits.S02E03.HDTV.x264-ASAP.srt', 'Suits.S02E04.HDTV.x264-ASAP.srt', 'Suits.S02E05.iNTERNAL.HDTV.x264-2HD.srt', 'Suits.S02E06.All.In.HDTV.x264-FQM.srt', 'Suits.S02E07.Sucker.Punch.PROPER.HDTV.x264-FQM.srt', 'Suits.S02E08.HDTV.x264-EVOLVE.srt', 'Suits.S02E09.HDTV.x264-ASAP.srt', 'Suits.S02E10.HDTV.x264-ASAP.srt', 'Suits.S02E11.HDTV.x264-ASAP.srt', 'Suits.S02E12.HDTV.x264-ASAP.srt', 'Suits.S02E13.REPACK.HDTV.x264-2HD.srt', 'Suits.S02E14.HDTV.x264-ASAP.srt', 'Suits.S02E15.HDTV.x264-ASAP.srt', 'Suits.S02E16.HDTV.x264-2HD.srt', 'Suits.S03E01.480pHDTV.x264-mSD.srt', 'Suits.S03E02.720pHDTV.x264-mSD.srt', 'Suits.S03E03.480pHDTV.x264-mSD.srt', 'Suits.S03E04.480pHDTV.x264-mSD.srt', 'Suits.S03E05.480p.HDTV.x264-mSD.srt', 'Suits.S03E06.720p.HDTV.x264-mSD.srt', 'Suits.S03E07.HDTV.x264-mSD.srt', 'Suits.S03E08.480p.HDTV.x264-mSD.srt', 'Suits.S03E09.480p.HDTV.x264-mSD.srt', 'Suits.S03E10.HDTV.x264-mSD.srt', 'Terminator_2_Judgment_Day_1991_roNy.srt', 'The Big Bang Theory - 1x01 - Pilot (XOR).srt', 'The Big Bang Theory - 1x02 - The Big Bran Hypothesis (XOR).srt', 'The Big Bang Theory - 1x03 - The Fuzzy Boots Corollary (XOR).srt', 'The Big Bang Theory - 1x04 - The Luminous Fish Effect (XOR).srt', 'The Big Bang Theory - 1x05 - The Hamburger Postulate (XOR).srt', \"The IT Crowd - 1e01-Yesterday's Jam.srt\", 'The IT Crowd - 1e02-Calamity Jen.srt', 'The IT Crowd - 1e03-Fifty-Fifty.srt', 'The IT Crowd - 1e04-The Red Door.srt', 'The IT Crowd - 1e05-The Haunting of Bill Crouse.srt', 'The IT Crowd - 1e06-Aunt Irma Visits.srt', 'The Last Song 2010.srt', 'The Secret Life of Pets.en.srt', 'The Simpsons - 1x01 - Simpsons Roasting on an Open Fire.srt', 'The Simpsons - 1x02 -Bart the Genius.srt', \"The Simpsons - 1x03 - Homer's Odyssey.srt\", \"The Simpsons - 1x04 - There's No Disgrace Like Home.srt\", 'The Simpsons - 1x05 - Bart the General.srt', 'The Walking Dead-S01E01-Days Gone Bye.English.srt', 'The Walking Dead-S01E02-Guts.English.srt', 'The Walking Dead-S01E03-Tell It To The Frogs.English.srt', 'The Walking Dead-S01E04-Vatos.English.srt', 'The Walking Dead-S01E05-Wildfire.English.srt', 'The Walking Dead-S01E06-TS-19.English.srt', 'the.crown.s01e06.720p.webrip.hevc.x265.rmteam.ass', 'the.crown.s01e07.720p.webrip.hevc.x265.rmteam.ass', 'the.crown.s01e08.720p.webrip.hevc.x265.rmteam.ass', 'the.crown.s01e09.720p.webrip.hevc.x265.rmteam.ass', 'the.crown.s01e10.720p.webrip.hevc.x265.rmteam.ass', 'The.Green.Mile.EnSub.1999BD720p_SharePirate.srt', 'The.Grinch.2018.1080p.WEB-DL.X264.DD5.1-SeeHD.srt', 'The.Grinch.2018.REMUX.1080p.Blu-ray.AVC.TrueHD.DTS-HD.MA.7.1-LEGi0N.English.srt', 'The.Hollow.S01E01.720p.WEB.x264-EDHD.srt', 'The.Hollow.S01E02.720p.WEB.x264-EDHD.srt', 'The.Hollow.S01E03.720p.WEB.x264-EDHD.srt', 'The.Hollow.S01E04.720p.WEB.x264-EDHD.srt', 'The.Hollow.S01E05.720p.WEB.x264-EDHD.srt', 'The.Hollow.S01E06.720p.WEB.x264-EDHD.srt', 'The.Hollow.S01E07.720p.WEB.x264-EDHD.srt', 'The.Hollow.S01E08.720p.WEB.x264-EDHD.srt', 'The.Hollow.S01E09.720p.WEB.x264-EDHD.srt', 'The.Hollow.S01E10.720p.WEB.x264-EDHD.srt', 'The.Hollow.S02E01.Home.720p.NF.WEB-DL.DDP5.1.H.264-NTb.srt', 'The.Hollow.S02E02.Hollow.Games.720p.NF.WEB-DL.DDP5.1.H.264-NTb.srt', 'The.Hollow.S02E03.The.Return.720p.NF.WEB-DL.DDP5.1.H.264-NTb.srt', 'The.Hollow.S02E04.Puzzled.720p.NF.WEB-DL.DDP5.1.H.264-NTb.srt', 'The.Hollow.S02E05.Alchemy.720p.NF.WEB-DL.DDP5.1.H.264-NTb.srt', 'The.Hollow.S02E06.Dead.End.720p.NF.WEB-DL.DDP5.1.H.264-NTb.srt', 'The.Hollow.S02E07.Unbalanced.720p.NF.WEB-DL.DDP5.1.H.264-NTb.srt', 'The.Hollow.S02E08.Fang.720p.NF.WEB-DL.DDP5.1.H.264-NTb.srt', 'The.Hollow.S02E09.Fire.720p.NF.WEB-DL.DDP5.1.H.264-NTb.srt', 'The.Hollow.S02E10.Race.720p.NF.WEB-DL.DDP5.1.H.264-NTb.srt', 'The.Man.Called.Flintstone.1966.1080p.WEB-DL.DD+2.0.H.264-DAWN.srt', 'The.Mentalist.S01E01.720p.BluRay.x264.350MB-PaHe.in.srt', 'The.Mentalist.S01E02.720p.BluRay.x264.299MB-PaHe.in.srt', 'The.Mentalist.S01E03.720p.BluRay.x264.299MB-PaHe.in.srt', 'The.Mentalist.S01E04.720p.BluRay.x264.299MB-PaHe.in.srt', 'The.Mentalist.S01E05.720p.BluRay.x264.299MB-PaHe.in.srt', 'The.Notebook.DVDRip.XviD-DiAMOND.srt', 'The.Prestige.2006.2160p.BluRay.Remux.srt', 'The.Social.Network.2010.1080p.BluRay.DTS-HD.MA.5.1.x264-BluEvo.srt', 'The.Sound.of.Music.1965.WEBRip.iTunes.srt', 'The.True.Cost.2015.BluRay.720p.700MB.Ganool.com.srt', 'the.tudors.s01e01.dvdrip.xvid-wide.srt', 'the.tudors.s01e02.dvdrip.xvid-wide.srt', 'the.tudors.s01e03.dvdrip.xvid-wide.srt', 'the.tudors.s01e04.dvdrip.xvid-wide.srt', 'the.tudors.s01e05.dvdrip.xvid-wide.srt', 'The.Umbrella.Academy.S01E01.WEBRip.x264-ION10.srt', 'The.Umbrella.Academy.S01E02.WEBRip.x264-ION10.srt', 'The.Umbrella.Academy.S01E03.WEBRip.x264-ION10.srt', 'The.Umbrella.Academy.S01E04.WEBRip.x264-ION10.srt', 'The.Umbrella.Academy.S01E05.WEBRip.x264-ION10.srt', 'The.Umbrella.Academy.S01E06.WEBRip.x264-ION10.srt', 'The.Umbrella.Academy.S01E07.WEBRip.x264-ION10.srt', 'The.Umbrella.Academy.S01E08.WEBRip.x264-ION10.srt', 'The.Umbrella.Academy.S01E09.WEBRip.x264-ION10.srt', 'The.Umbrella.Academy.S01E10.WEBRip.x264-ION10.srt', 'The.Umbrella.Academy.S02E01.WEBRip.Netflix.en.srt', 'The.Umbrella.Academy.S02E02.WEBRip.Netflix.en.srt', 'The.Umbrella.Academy.S02E03.WEBRip.Netflix.en.srt', 'The.Umbrella.Academy.S02E04.WEBRip.Netflix.en.srt', 'The.Umbrella.Academy.S02E05.WEBRip.Netflix.en.srt', 'The.Umbrella.Academy.S02E06.WEBRip.Netflix.en.srt', 'The.Umbrella.Academy.S02E07.WEBRip.Netflix.en.srt', 'The.Umbrella.Academy.S02E08.WEBRip.Netflix.en.srt', 'The.Umbrella.Academy.S02E09.WEBRip.Netflix.en.srt', 'The.Umbrella.Academy.S02E10.WEBRip.Netflix.en.srt', 'The.Umbrella.Academy.S03E01.720p.WEB.h264-KOGi-en-forced.srt', 'The.Umbrella.Academy.S03E01.720p.WEB.h264-KOGi.srt', 'The.Umbrella.Academy.S03E02.720p.WEB.h264-KOGi.srt', 'The.Umbrella.Academy.S03E03.720p.WEB.h264-KOGi.srt', 'The.Umbrella.Academy.S03E04.720p.WEB.h264-KOGi.srt', 'The.Umbrella.Academy.S03E05.720p.WEB.h264-KOGi.srt', 'The.Umbrella.Academy.S03E06.720p.WEB.h264-KOGi-en-forced.srt', 'The.Umbrella.Academy.S03E06.720p.WEB.h264-KOGi.srt', 'The.Umbrella.Academy.S03E07.720p.WEB.h264-KOGi-en-forced.srt', 'The.Umbrella.Academy.S03E07.720p.WEB.h264-KOGi.srt', 'The.Umbrella.Academy.S03E08.720p.WEB.h264-KOGi-en-forced.srt', 'The.Umbrella.Academy.S03E08.720p.WEB.h264-KOGi.srt', 'The.Umbrella.Academy.S03E09.720p.WEB.h264-KOGi.srt', 'The.Umbrella.Academy.S03E10.720p.WEB.h264-KOGi.srt', 'The.Vampire.Diaries.S01E01.HDTV.XviD-FQM.srt', 'The.Vampire.Diaries.S01E02.HDTV.XviD-FQM.srt', 'The.Vampire.Diaries.S01E03.HDTV.XviD-FQM.srt', 'The.Vampire.Diaries.S01E04.HDTV.XviD-FQM.srt', 'The.Vampire.Diaries.S01E05.HDTV.XviD-NoTV.srt', 'The.Wolf.of.Wall.Street.2013.720p.BluRay.x264.Ganool.srt', 'TheGreatestShowman.brrip.2017.1080p.srt', 'The_blind_side(2009).srt', 'The_break-up(2006).srt', 'The_cabin_in_the_woods(2012).srt', 'The_Devil_Wears_Prad.srt', 'The_fault_in_our_stars(2014).srt', 'The_Fundamentals_of_Caring(2016).srt', 'The_Ghost_Writer.srt', 'The_graduate(1967).srt', 'The_greatest_showman(2017).srt', 'The_hangover(2009).srt', 'The_holiday(2006).srt', 'The_Intern(2015).srt', 'The_invisible_man(2020).srt', 'The_jungle_book(2016).srt', 'The_kings_speech(2010).srt', 'The_Legend_of_Tarzan(2016).srt', 'The_lion_king(1994).srt', 'The_lord_of_the_rings(2001).srt', 'The_man_called_Flintstone(1966).srt', 'The_secret_life_of_Walter_Mitty(2013).srt', 'The_Shawshank_redemption(1994).srt', 'The_social_network(2010).srt', 'The_terminal(2004).srt', 'The_terminator(1984).srt', 'The_theory_of_everything(2014).srt', 'The_usual_suspects(1995).srt', 'Thor.Love.and.Thunder.2022.1080p.WEBRip.X264.DD5.1.srt', 'Titanic(1997).srt', 'Toy_story(1995).srt', 'Twilight(2008).srt', 'Up.2009.720p.BluRay.x264.YIFY.srt', \"Valentine's.Day.2010.Subtitles.YIFY.srt\", 'Venom(2018).srt', 'vikings.s01e01.repack.hdtv.x264-2hd.srt', 'Virgin.River.S01E01.INTERNAL.720p.WEB.x264-STRiFE.srt', 'Virgin.River.S01E02.INTERNAL.720p.WEB.x264-STRiFE.srt', 'Virgin.River.S01E03.INTERNAL.720p.WEB.x264-STRiFE.srt', 'Virgin.River.S01E04.INTERNAL.720p.WEB.x264-STRiFE.srt', 'Virgin.River.S01E05.INTERNAL.720p.WEB.x264-STRiFE.srt', 'Virgin.River.S01E06.INTERNAL.720p.WEB.x264-STRiFE.srt', 'Virgin.River.S01E07.INTERNAL.720p.WEB.x264-STRiFE.srt', 'Virgin.River.S01E08.INTERNAL.720p.WEB.x264-STRiFE.srt', 'Virgin.River.S01E09.INTERNAL.720p.WEB.x264-STRiFE.srt', 'Virgin.River.S01E10.INTERNAL.720p.WEB.x264-STRiFE.srt', 'WALL-E.2008.BluRay.1080p.5.1CH.x264.Ganool.com.srt', 'Warm_bodies(2013).srt', 'Westworld_scenes_of_Dr_Robert_Ford.srt', 'We_are_the_Millers(2013).srt', 'While_You_Were_Sleeping(1995).srt', 'Xena Warrior Princess - S01E01 - Sins of the Past.eng.srt', 'Xena Warrior Princess - S01E02 - Chariots of War.eng.srt', 'Xena Warrior Princess - S01E03 - Dreamworker.eng.srt', 'Xena Warrior Princess - S01E04 - Cradle of Hope.eng.srt', 'Xena Warrior Princess - S01E05 - The Path Not Taken.eng.srt', 'Xena Warrior Princess - S01E06 - The Reckoning.eng.srt', 'Xena Warrior Princess - S01E07 - The Titans.eng.srt', 'Xena Warrior Princess - S01E08 - Prometheus.eng.srt', 'Xena Warrior Princess - S01E09 - Death in Chains.eng.srt', 'Xena Warrior Princess - S01E10 - Hooves & Harlots.eng.srt', 'z srt23 uk-bun Gullivers.Travels.1939.720p.BluRay.x264-CiNEFiLE.srt', 'Zootopia(2016).srt']\n"
     ]
    }
   ],
   "source": [
    "#опрелеляем файлы в директории\n",
    "path = 'C:/Users/angel/project_english_school/Subtitles_all_new/rar'\n",
    "dir_list = os.listdir(path)\n",
    "print('Файлы и директории в ', path, ':')\n",
    "print(dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8d33b070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#создаем датасет из субтитров и названия фильмов\n",
    "results = defaultdict(list)\n",
    "name = defaultdict(list)\n",
    "\n",
    "for file in dir_list:\n",
    "    sub = pysrt.open(path+'/'+file, encoding = 'latin-1')\n",
    "    results['subtitles'].append(sub.text)\n",
    "    name['Movie'].append(file.replace('.srt',''))\n",
    "data = pd.DataFrame(results).join(pd.DataFrame(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "217a45a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#считаем данные по классам фильмов\n",
    "movies_labels = pd.read_excel('C:/Users/angel/project_english_school/movies_labels_2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0f3073b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#создаем первый датасет\n",
    "\n",
    "data1 = data.merge(movies_labels, 'inner', left_on = 'Movie', right_on = 'Movie').drop('id', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "114369c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файлы и директории в  C:/Users/angel/project_english_school/Subtitles_all :\n",
      "['A2', 'B1', 'B2', 'C1', 'Subtitles']\n"
     ]
    }
   ],
   "source": [
    "path = 'C:/Users/angel/project_english_school/Subtitles_all'\n",
    "dir_list = os.listdir(path)\n",
    "print('Файлы и директории в ', path, ':')\n",
    "print(dir_list)\n",
    "\n",
    "\n",
    "results = defaultdict(list)\n",
    "name = defaultdict(list)\n",
    "label = defaultdict(list)\n",
    "\n",
    "for file in dir_list:\n",
    "    if file in ['A2', 'B1', 'B2', 'C1']:\n",
    "        dir_list_films = os.listdir(path+'/'+file)\n",
    "        for film in dir_list_films:\n",
    "            sub = pysrt.open(path+'/'+file + '/'+ film, encoding = 'latin-1')\n",
    "            results['subtitles'].append(sub.text)\n",
    "            name['Movie'].append(film.replace('.srt',''))\n",
    "            label['Level'].append(file)\n",
    "        \n",
    "data2 = pd.DataFrame(results).join(pd.DataFrame(name)).join(pd.DataFrame(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "82c449ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data1, data2]).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "25eed993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subtitles</th>\n",
       "      <th>Movie</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is the story of Bridget and Annie,\\nwho s...</td>\n",
       "      <td>01_Extra_English_-_Hectors_arrival</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is the story of two girls\\nwho share a fl...</td>\n",
       "      <td>02_Extra_English_-_Hector_goes_shopping.Vie_Syned</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is the story of two girls\\nwho share a fl...</td>\n",
       "      <td>03_Extra_English_-_Hector_has_a_date.Eng_Syned</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is the story of two girls\\nwho share a fl...</td>\n",
       "      <td>04_Extra_English_-_Hector_looks_for_a_job.Vie_...</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is the story of Bridget and Annie,\\nwho s...</td>\n",
       "      <td>05_Extra_English_-_A_star_is_born.Eng_Syned</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           subtitles  \\\n",
       "0  This is the story of Bridget and Annie,\\nwho s...   \n",
       "1  This is the story of two girls\\nwho share a fl...   \n",
       "2  This is the story of two girls\\nwho share a fl...   \n",
       "3  This is the story of two girls\\nwho share a fl...   \n",
       "4  This is the story of Bridget and Annie,\\nwho s...   \n",
       "\n",
       "                                               Movie Level  \n",
       "0                 01_Extra_English_-_Hectors_arrival    A1  \n",
       "1  02_Extra_English_-_Hector_goes_shopping.Vie_Syned    A1  \n",
       "2     03_Extra_English_-_Hector_has_a_date.Eng_Syned    A1  \n",
       "3  04_Extra_English_-_Hector_looks_for_a_job.Vie_...    A1  \n",
       "4        05_Extra_English_-_A_star_is_born.Eng_Syned    A1  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af5a46b1",
   "metadata": {},
   "source": [
    "## 2.EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6aa76306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A1', 'B1', 'A2', 'A2/A2+', 'B2', 'C1', 'B1, B2', 'A2/A2+, B1',\n",
       "       'B2, C1'], dtype=object)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#исследуем типы значений целевого признака\n",
    "data['Level'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "400f1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#уберем стоп слова, скорректируем данные субтитров, проведем лемматизацию, добавим дополнительные параметры для модели, скорректируем целевой признак\n",
    "\n",
    "HTML = r'<.*?>' # html тэги меняем на пробел\n",
    "TAG = r'{.*?}' # тэги меняем на пробел\n",
    "COMMENTS = r'[\\(\\[][A-Za-z ]+[\\)\\]]' # комменты в скобках меняем на пробел\n",
    "UPPER = r'[[A-Za-z ]+[\\:\\]]' # указания на того кто говорит (BOBBY:)\n",
    "LETTERS = r'[^a-zA-Z\\'.,!? ]' # все что не буквы меняем на пробел \n",
    "SPACES = r'([ ])\\1+' # повторяющиеся пробелы меняем на один пробел\n",
    "DOTS = r'[\\.]+' # многоточие меняем на точку\n",
    "SYMB = r\"[^\\w\\d'\\s]\" # знаки препинания кроме апострофа\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm',exclude=[\"tok2vec\", \"parser\", \"ner\", \"attrbute_ruler\"])\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "def clean_subs(subs):\n",
    "    subs = subs[1:] # удаляем первый рекламный субтитр\n",
    "    txt = re.sub(HTML, ' ', subs) # html тэги меняем на пробел\n",
    "    txt = re.sub(COMMENTS, ' ', txt) # комменты в скобках меняем на пробел\n",
    "    txt = re.sub(UPPER, ' ', txt) # указания на того кто говорит (BOBBY:)\n",
    "    txt = re.sub(LETTERS, ' ', txt) # все что не буквы меняем на пробел\n",
    "    txt = re.sub(DOTS, r'.', txt) # многоточие меняем на точку\n",
    "    txt = re.sub(SPACES, r'\\1', txt) # повторяющиеся пробелы меняем на один пробел\n",
    "    txt = re.sub(SYMB, '', txt) # знаки препинания кроме апострофа на пустую строку\n",
    "    txt = re.sub('www', '', txt) # кое-где остаётся www, то же меняем на пустую строку\n",
    "    txt = txt.lstrip() # обрезка пробелов слева\n",
    "    txt = txt.encode('ascii', 'ignore').decode() # удаляем все что не ascii символы   \n",
    "    txt = txt.lower() # текст в нижний регистр\n",
    "    return txt\n",
    "\n",
    "def token_drop_stop(subs):\n",
    "        \n",
    "    new_tokens = tokenizer(subs)\n",
    "    drop_stops = [w for w in new_tokens if w not in stopwords]  \n",
    "    res_text = ''.join([token.lemma_ for token in nlp(str(drop_stops))])\n",
    "    return res_text\n",
    "\n",
    "def new_features(subs):\n",
    "     new_tokens = tokenizer(subs)\n",
    "     number = 0\n",
    "     count = 0\n",
    "     for w in new_tokens:\n",
    "         number += len(w)\n",
    "         count += 1\n",
    "     return number/count\n",
    "\n",
    "def level_update(row):\n",
    "    global new_level\n",
    "    if row == 'B1, B2':\n",
    "        new_level = 'B1'\n",
    "    elif row == 'A2/A2+':\n",
    "        new_level = 'A2'\n",
    "    elif row == 'A2/A2+, B1':\n",
    "        new_level = 'A2'\n",
    "    elif row == 'B2, C1':\n",
    "        new_level = 'B2'\n",
    "    else: new_level = row\n",
    "    return new_level\n",
    "\n",
    "def check_min_d(row):\n",
    "    global x\n",
    "    if row == 'A1':\n",
    "        x = 1\n",
    "    elif row == 'A2':\n",
    "        x = 2\n",
    "    elif row == 'B1':\n",
    "        x = 3\n",
    "    elif row == 'B2':\n",
    "        x = 4\n",
    "    else: x = 5\n",
    "\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "618d6252",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Level'] = data['Level'].apply(lambda row: level_update(row))\n",
    "data['subtitles'] = data['subtitles'].apply(lambda row: clean_subs(row)).apply(lambda row: token_drop_stop(row))\n",
    "#data['avg_word_length'] = data['subtitles'].apply(lambda row: new_features(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "312999a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['test'] = data['Level'].apply(lambda row: check_min_d(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bbc2551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = copy.deepcopy(data.sort_values(by = ['test']).drop_duplicates(keep = 'first').drop('test', axis = 1).reset_index().drop('index', axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9885e609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subtitles</th>\n",
       "      <th>Movie</th>\n",
       "      <th>Level</th>\n",
       "      <th>avg_word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[his,is,the,story,of,bridget,and,annie,who,sha...</td>\n",
       "      <td>01_Extra_English_-_Hectors_arrival</td>\n",
       "      <td>A1</td>\n",
       "      <td>2241.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[his,is,the,story,of,bridget,and,annie,who,sha...</td>\n",
       "      <td>16_Extra_English_-_Uncle_Nick.Vie_Syned</td>\n",
       "      <td>A1</td>\n",
       "      <td>5329.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[his,is,the,story,of,bridget,and,annie,who,sha...</td>\n",
       "      <td>17_Extra_English_-_Cyber_Stress.Eng_Syned</td>\n",
       "      <td>A1</td>\n",
       "      <td>5541.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[his,is,the,story,of,bridget,and,annie,who,sha...</td>\n",
       "      <td>18_Extra_English_-_Just_the_ticket.Vie_Syned</td>\n",
       "      <td>A1</td>\n",
       "      <td>2584.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[his,is,the,story,of,bridget,and,annie,who,sha...</td>\n",
       "      <td>19_Extra_English_-_Kung_fu_fighting.Eng_Syned</td>\n",
       "      <td>A1</td>\n",
       "      <td>5569.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           subtitles  \\\n",
       "0  [his,is,the,story,of,bridget,and,annie,who,sha...   \n",
       "1  [his,is,the,story,of,bridget,and,annie,who,sha...   \n",
       "2  [his,is,the,story,of,bridget,and,annie,who,sha...   \n",
       "3  [his,is,the,story,of,bridget,and,annie,who,sha...   \n",
       "4  [his,is,the,story,of,bridget,and,annie,who,sha...   \n",
       "\n",
       "                                           Movie Level  avg_word_length  \n",
       "0             01_Extra_English_-_Hectors_arrival    A1           2241.0  \n",
       "1        16_Extra_English_-_Uncle_Nick.Vie_Syned    A1           5329.0  \n",
       "2      17_Extra_English_-_Cyber_Stress.Eng_Syned    A1           5541.0  \n",
       "3   18_Extra_English_-_Just_the_ticket.Vie_Syned    A1           2584.0  \n",
       "4  19_Extra_English_-_Kung_fu_fighting.Eng_Syned    A1           5569.0  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93ba6f4b",
   "metadata": {},
   "source": [
    "## 2.Разбиение на трейн и тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "21a4d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "#фиксируем переменные\n",
    "RANDOM_STATE = 12345\n",
    "TEST_SIZE = 0.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8aa336ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Относительная величина элементов в трейне subtitles          0.699288\n",
      "avg_word_length    0.699288\n",
      "dtype: float64\n",
      "Относительная величина элементов в тесте subtitles          0.300712\n",
      "avg_word_length    0.300712\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#разбиваем датасет на трейн, тест, валид\n",
    "X = data.drop(['Movie', 'Level'],axis=1)\n",
    "Y = data['Level']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,random_state = RANDOM_STATE, test_size = TEST_SIZE)\n",
    "\n",
    "\n",
    "print('Относительная величина элементов в трейне',X_train.count()/X.count())\n",
    "print('Относительная величина элементов в тесте',X_test.count()/X.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb0a01a7",
   "metadata": {},
   "source": [
    "## 3.Создание пайплайна по логистической регресии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "36eab6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\angel\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# создаем пайплайн логистической регресии и обучим модель\n",
    "logistic_reg = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='char', ngram_range = (2,10) ) ),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression(n_jobs=4,C=3e5, solver='saga', \n",
    "                               multi_class='multinomial',\n",
    "                               class_weight = 'balanced',\n",
    "                               max_iter=500,\n",
    "                               random_state=RANDOM_STATE)),\n",
    "])\n",
    "logistic_reg.fit(X_train['subtitles'], Y_train)\n",
    "pred = logistic_reg.predict(X_test['subtitles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c0f46631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A1       1.00      0.83      0.91        12\n",
      "          A2       0.74      0.91      0.82        54\n",
      "          B1       0.55      0.46      0.50        24\n",
      "          B2       0.75      0.80      0.78        61\n",
      "          C1       1.00      0.44      0.62        18\n",
      "\n",
      "    accuracy                           0.75       169\n",
      "   macro avg       0.81      0.69      0.72       169\n",
      "weighted avg       0.76      0.75      0.74       169\n",
      "\n",
      "F1 Score: 0.7427837776950202\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, pred))\n",
    "print(f\"F1 Score: {f1_score(Y_test, pred, average='weighted')}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57eb5315",
   "metadata": {},
   "source": [
    "## 4.Обучение модели Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "986e4100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(train_pool, test_pool, **kwargs):\n",
    "    model = CatBoostClassifier(task_type = 'CPU', iterations = 5000,\n",
    "                               eval_metric = 'TotalF1', od_type = 'Iter', random_state=RANDOM_STATE,\n",
    "                               od_wait=500, **kwargs)\n",
    "    return model.fit(train_pool, eval_set = test_pool,\n",
    "                     verbose = 100, plot=True,\n",
    "                     use_best_model = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4bab6edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pool = Pool(data=X_train, label=Y_train, \n",
    "                  text_features=['subtitles',])\n",
    "valid_pool = Pool(data=X_test, label=Y_test, \n",
    "                  text_features=['subtitles',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6713d540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8838639490c0448ca324a608180dd052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.2954271\ttest: 0.2749083\tbest: 0.2749083 (0)\ttotal: 149ms\tremaining: 12m 24s\n",
      "100:\tlearn: 0.5798427\ttest: 0.3451250\tbest: 0.3490481 (47)\ttotal: 637ms\tremaining: 30.9s\n",
      "200:\tlearn: 0.6545791\ttest: 0.3362460\tbest: 0.3500927 (172)\ttotal: 1.41s\tremaining: 33.8s\n",
      "300:\tlearn: 0.6736347\ttest: 0.3262126\tbest: 0.3500927 (172)\ttotal: 2.31s\tremaining: 36s\n",
      "400:\tlearn: 0.6986577\ttest: 0.3085549\tbest: 0.3500927 (172)\ttotal: 3.15s\tremaining: 36.1s\n",
      "500:\tlearn: 0.7404118\ttest: 0.3075692\tbest: 0.3500927 (172)\ttotal: 3.9s\tremaining: 35s\n",
      "600:\tlearn: 0.7650039\ttest: 0.2992522\tbest: 0.3500927 (172)\ttotal: 4.56s\tremaining: 33.4s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 0.3500927342\n",
      "bestIteration = 172\n",
      "\n",
      "Shrink model to first 173 iterations.\n"
     ]
    }
   ],
   "source": [
    "model = fit_model(train_pool, valid_pool, learning_rate=0.25,\n",
    "                  dictionaries = [{\n",
    "                      'dictionary_id':'Word',\n",
    "                      'max_dictionary_size': '50000'\n",
    "                  }],\n",
    "                 feature_calcers = ['BoW:top_tokens_count=10000'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd25fe35",
   "metadata": {},
   "source": [
    "## Сохраним модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "08ac99eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\angel\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#согласно метрике наилучшая модель - логистическая регрессия\n",
    "#обучим модель на всем датасете\n",
    "#затем сохраним модель для дальнейшего использования\n",
    "\n",
    "logistic_reg.fit(X['subtitles'], Y)\n",
    "\n",
    "with open('./main.pcl', 'wb') as model_file:\n",
    "    dump(logistic_reg,model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
